= Lessons from Six Months on a Software Engineering Team

In mid April 2020 I was put onto a team of software engineers. The team was in place from around November 2019, and there were some problems with it. 

With a strong caution against mistaking correlation for causation, during the six months I was on the team things got much better. By late September 2020 though it certainly wasn't perfect, it was working pretty well. We had decent velocity, little technical debt, and were delivering things that make our customers happy. 

In October 2020 I left the team. 

This post is a retrospective about what we did and what I learned during the six months.

I emphasize from the outset that everyone involved in this team at the least competent, and in most cases extremely talented. I strongly believe that the problems I'll discuss were results of the situation we had put ourselves in, _not_ of the people involved. This is supported by the fact that we got to a much more healthy state largely without any changes in personnel.

== The state of the team as of April 2020

The team structure we had on 14 April was this:

* Our head of software engineering, the de facto product owner.
* Our lead cloud architect.
* A representative from Contractor A, who was the front-man for a team of offshore software engineers.
* Two engineers from Contractor B, working on a separate product from Contractor A.
* Two members of our Data Team, who were providing data inputs which would be used by Contractor A.
* Me, a business analyst, involved for the last month, but only now taking on the team properly. 
* The previous business analyst, whose was the defacto 'scrum master' for the team, who I was taking over from

So eight people going forward.

The two data team members were spending a small proportion of their time on this team's work. They had a lot of other responsibilities and projects. 

The head of software engineering obviously had many other responsibilities managing and administering the wider SE team, and was spending a moderate portion of his time on this team.

This team was working on two products, both greenfield. The first was an SPA which was intending to generate and present datasets of business information, with the data being provided by the data team's data-warehouse. This would sit on top of a 'shell' platform with the intention of hosting other SPAs in the future.

As of 14 April 2020, the first product had not been fully put in production, and was not close to being ready to do so.

The second was a cloud based calculator for a common business metric. This had only recently started.

The dynamic of the team was bad. In particular our cloud architect and the Contractor B architect were clashing over how to build things, and the working styles of the two were clearly incompatible. The discussions in standup and design sessions often devolved into arguments, occasionally extremely heated.

The Contractor B product had much less day-to-day input and guidance from us. The product spec had largely been defined upfront (partly by me, and in retrospect, very insufficiently) and handed over with a naive understanding (also by me) that this would be sufficient. I sort of thought they had enough and could just get on with it. I should've known better, but it was convenient for me to think that because I was concentrating on other things at the time.

There was also a less serious problems between the data team and Contractor A. The data team were responsible for delivering datasets and definitions to Contractor A for them to build into the product. The data team were not delivering these things quickly, or in the form that the Contractor needed, nor were Contractor A and the data team openly communicating about it. Contractor A in my assessment had given up and was not making efforts to resolve this, and the data team members seemed to me to take this as a green flag that it wasn't important to focus on this, and they were free to work on their own products, which they were more directly invested in. As a result, the work wasn't getting done. There were occasional comments in standup, but the issue wasn't being addressed directly.

Contractor B was committing code directly into our repositories. We faced a difficult choice when it came to maintaining control over the code. Our first option was to have one of our in-house devs review the PRs. However, we had at the time exactly one in house dev, and that was our head of software engineering. He was heavily involved in the other product, not to mention his work outside the team. All these things considered, there was not the bandwidth available to do all of these things. Consequently, the PR reviews were a bottleneck in the process, and the reviews were necessarily limited in scope.

Our second option was to have a non-software developer member of the team (me) 'review' the PR. This removes the bottleneck, but obviously results in less scrutiny of the code. We had gone with the first option.

Contractor A, on the other hand, were committing code into their own repositories, and only periodically pushing it to our repos. The rationale for this was partly that the model of work we had agreed on with them was influenced by the traditional 'software build contract' that Contractor A was used to working in, where the client provides requirements and the contractor builds the product, and provides the finished product to the client without the client being involved with the code reviews during the build process. Contrast this with Contractor B, where the devs are effectively 'in-sourced', that is their working method is largely indistinguishable from an in-house developer. 

Contractor A was strongly in favour of this approach, to the point I would say of making it a condition of the contract. They claimed, and I believe them, that changing to the other model would be highly disruptive to their workflow. Another reason we went with this model is that the offshore devs did not speak very good English. Thus, any comments we did make on PRs would not be understood by them anyway, so the value would be extremely limited and would probably only cause confusion.

Lastly, we had a very large amount of technical debt on the SPA product, particularly around the infrastructure. In the interests of getting the product out, we were building most of the cloud infrastructure manually. There was little in the way of CICD, and no automated builds or deployments.

The cloud architect and the outgoing business manager had identified this as a significant issue, and together they had put together a plan to 'back fill' the IAC, though this hadn't started yet.

There was also a lack of clarity around who was actually responsible for building this infrastructure: Contractor A or our cloud architect. At this point we hadn't directly addressed the responsibility aspect. 

I'll summarize the problems here, and then we can dig into some of them more deeply and talk about what the team tried to do to resolve them.

* The team was too large
* The team had two different focuses at once - or rather, there was no focus. Focuses is an oxymoron. 
* Several of the team members were dedicating only a small amount of their overall time to the team, leading to inefficiencies due to divided attention and context switching
* The roles of the team members were not exactly clear cut - though this is a nuanced problem that I would caution against jumping to conclusions about, as I'll discuss
* There were multiple bad dynamics between individuals on the team that were impeding progress
* There was a capacity constraint on in-house developers which meant that the code reviews were a bottleneck
* There was not enough ongoing guidance on the product for the Contractor B developers, and the initial product spec wasn't good enough
* The method for flowing code between us and Contractor A was causing issues
* We had a lot of technical debt caused by building things manually, which would require significant amount of time to resolve.

To finish up this summary of the position on 14 April 2020, a description of the first spring planning session I ran that day. It was in a word a disaster. No one talked. There was no debate. There were long periods of awkward silence, and several snide remarks. I did not feel very positive at the end of it. Several people mentioned to me afterwards how excruciating it was. I told them I expected it to get better, but I had no idea if that was true.

=== Contractor B

The first thing to do was create a distance between the Contractor B devs and the rest of the team. Direct interaction wasn't working, and we were getting into a vicious cycle of back and forth accusations, to the point where there was no constructive dialogue possible. I was set up as the main point of interaction with the contractors, and any discussions between them and the rest of the team would be kept to a minimum, but carefully structured and moderated where they were unavoidable.

This focus was also intended to solve the issue of not having enough day-to-day guidance for contractors, since I was now spending much more time with them.

Where there were things that needed to be communicated between the groups, I tried to act as the messenger, and tone things down where necessary. I also talked a lot with the groups individually, partly to give them an outlet for frustration, and partly to try to try and appeal to them to keep things civil when they did talk directly.

The relationship issue was never solved, but cutting down on the communications did help. To my surprise we achieved everything that was set out in the Statement of Work, on time an on budget. But it wasn't much fun.

In general having an intermediary between groups of developers is not a good thing, and I think that as a general rule you should try to do the opposite - you should be creating an atmosphere where the developers can have real back-and-forth debates, and give their opinions openly and honestly.

Even so, I would conclude that this strategy of creating distance was the right one in this case. It provided the demilitarized zone between the groups, a space where tempers could cool and we could actually get some work done. 

We were lucky that what was needed was a short term fix. We only needed to maintain this state for a couple of months. In the situation where the groups would have to work together for a longer period, I would push for a more permanent divorce and a reshuffle of the team if possible. I think it would be ineffective to maintain the demilitarized-zone model over the long term, and optimistic to say that the groups can be reintegrated and get back to a good relationship.

=== Contractor A and the data team

The solution here was also fairly simple.

The communication channel between the data team and the contractor had to be reestablished, and the feedback loop of work being iterated over shortened. The contractor needed to change his attitude from 'I need this this stuff but you're not giving it to me, so whatever - clearly not important to you so not important to me' to being a partner in getting this job done. 

The data person needed to have the work he was expected to deliver broken down into more manageable chunks and have clear expectations set about when he was expected to deliver each chunk. These were two good and talented people who had established a bad dynamic, and after a few meetings where the tone was kept light and constructive, relations were quickly normalized and the work completed. My role here was merely to ensure the work was properly chunked and to set up a space where the two could have a dialogue. I also took on some of the testing and prompted some of the discussion to set an example of the feedback loop that was needed.

Again in practice this was only required for a short period since the once the work was done there wasn't much else required from the data team. But in this case I feel much more comfortable suggesting that this is a pattern that can be maintained over the long term. In fact, when we were later required to re-engage with the data team, the process was the same and was largely seamless.

=== Team size, focus and part-time team members

This issue was solved by time. The Contractor B program ended in June, meaning we were entirely focused on a single product and we had dropped two members from the team. The data team members were no longer in the critical path of development for the SPA and usually no longer on the stand-ups, removing two more.

In the end we had pared things down to a core team of me, the head of software engineering, the cloud architect and the Contractor A front-man. There was still some dilution of attention in that and I was working on a couple of other projects, and the head of software engineering still had a large number of other responsibilities, but on the whole it was much, much better.

If we hadn't been fortunate in the natural run-off here, I would have suggested two changes:

* The second focus be entirely separated from the first, creating a core team for that of just me and the two Contractor B developers
* That the data element be asked to provide only one resource to the team, that to the extent possible they be dedicated exclusively to the product, and that we make an effort to load as much work as we required them to do into a single iteration so as to time-box their involvement, rather than spread it out over several iterations and have them with one foot in the team and one foot out. 

=== CICD and technical debt

I won't say too much about this, since the plan to resolve it had already been put in place by my predecessor and the cloud architect, and this was largely resolved by him gradually building out the infrastructure over the six months.

The ongoing minor things we put in place to enable this were:

* make it explicit that this was a high priority
* block for the cloud architect so he could focus on it
* make sure that all the elements were represented by backlog items
* make sure that, prior to starting a new sprint, there were always TD pay-down tickets at the top of the backlog
* commit to bringing in some of those tickets to every sprint, regardless of what other directions we were being pulled in 
* ensuring that we were very explicit on the dependencies on Contractor A in terms of things the architect needed, that we knew when they would be blockers, and that we were talking about concrete times when they were expected.

As a team, we recognized the amount of technical debt we had built up. In particular the cut-over from manual infrastructure to IAC in production was very nerve-wracking and should absolutely be avoided. I got a new appreciation of the costs of punting on day-1 CICD and would be extremely hesitant to repeat it.

=== The Contractor A repo model

Let me say upfront we never really solved this, though we did improve it.

As a reminder, the issue here is that for reasons stated above Contractor A was committing code into their own repos, and only periodically pushing to ours for PRs. This created the following problems for us

* We were not able to comment on code in a timely manner.
* To a large degree it removes our traceability between Jira ticket and commit/PR - a vital element for effective software product management, especially in a CICD environment.
* It increased the feedback loop since we could only test and deploy the new code when it was pushed to us. In effect we were increasing the batch size of tickets in progress at any given time.
* It was administratively burdensome for the Contractor A front-man, who was manually syncing the two sets of repos
* It meant that we, ourselves, can't commit directly to our own repos, since any changes would be overridden by the next push from the Contractor. Though due to the nature of the product it was unlikely we would ever want to do this. 
* As a minor extension of the last point, it meant that we couldn't use Github's 'dependabot' to automatically handle the bumping of library dependencies which had security holes. Instead the Contractor had to make them on their end. 

I want to be clear that I wouldn't characterize the decision to use this model as a _mistake_. I think you can definitely make a strong case for the 'commit directly to our repos' model, but the pros and cons, which we debated on team, are pretty close in my view. We've never been explicit about it but I'm pretty sure if you polled the team members both sides would have supporters. 

We talked about this on-team a lot, and we did end up taking steps to minimize the cons of this approach. The Contractor wrote some code that removed the admin burden. We started using Jira releases better to create more visibility into the 'what was in this release' problem. We modified the kanban board to make it clearer what was and wasn't in our repos, and what in our repos and ready to deploy. We got more efficient about testing 'batches' of tickets at a time.

== The state of the team as of September 2020

So where did all this leave us as of September 2020?

First with regard to products, as mentioned the product being built by Contractor B was completed on time and on budget. It is not a perfect product, there are parts of it, especially on the infrastructure side, that need improvement. But overall it is a complete, cohesive product, in production, and I think we can be pleased with the result, especially since early indications were rather negative. 

Sadly due to subsequent events and reprioritizations, the product is not being actively used by the business yet. But when the time comes for someone to call it, we can be confident that it will do its job. 

The experience was very ugly, and not something I would like to go through again. Or at least, I would do things rather differently, with different people.

The SPA product being worked on with Contractor A was launched in June, and has been working well ever since. Initial uptake has been solid, with the usage of the legacy products it replaced being reduced to nil. 

Since initial launch we've added several new features. By September we had great momentum, with feature requests from the business being started, built and delivered in a single iteration.

The product is essentially CICD complete, and deployments became increasingly simple and stress free. I counted over 50 separate Jira releases pushed to production in the 4 months from June to September, and of those I can recall only one, early on, that broke production (though there were a few issues at the start caused by upstream processes).

In the last month I've had the opportunity to hear about feature requests from the business, to put them in a sprint, and then two weeks later to demo them to the business and say 'this is in production, you can use it now, go wild'. I have heard several comments along the lines of "I'm amazed how quickly you turned this around", "This is much further ahead than I thought", and "Wait, we can do this _right now_?", which I haven't heard before. 

When I compare my initial hot-mess sprint planning session in April to the way we communicate now, it's night and day. The team have things to say and are comfortable saying them and eager to contribute. Meetings are infrequent, informal, and effective, though occasionally devolve into rambling. People are settled into a nice balance. They largely know what is expected from them, and they know what to expect from the others. 

When you read about the early days of the Agile movement, before it was turned into an industry, the things the initiators talk about are small, self-organizing teams, delivering working software frequently and delighting customers. I feel like we had approached that at the end of September.  

Enough gushing, but it's fair to say that the team is in a much better place than it was 6 months ago, and it is _easily_ the most effective group I've been a part of. 

On to the more negative things. I'll skip over the minor points I've mentioned like the Contractor A repo issue.

Firstly, the team is in a good place right now, but that only really happened recently, in the few iterations leading up to the end of September. Even assuming this would continue and would not have been a temporary thing even absent of external forces, when you count from the actual kick off of the team (November 19), that's nearly a _full year_ from team formation to the level of effectiveness we should be aspiring to, and six months even from when I joined the team. 

This is with what I would consider our A-team, the best people we could've possibly brought to the table. Either we took much too long, or it's a very worrying indication for future teams we might want to spin up. And honestly I struggle to think of anything we could've done that would've significantly cut down this time to something less than, say, six months.  

Second, we have been lucky to a large degree that what we have been delivering, while fairly technically complicated and certainly totally new to us, has not had a large amount of business domain complexity. It has been comparatively generic functionality that we don't need a lot of domain expertise or even real two-way interaction with the business experts to understand, design and build. Of course we've had conversations with business users, for the scoping of their request, and then delivery of it on the other end of the sprint. But as this would suggest, these are fairly small pieces of relatively isolated functionality.

When we engage with a business group who have a request that has some real complexity to the domain, and will require a new application that will be developed over several iterations, even in the best of circumstances it would be a real challenge to the team to step up and work out the patterns of behavior necessary on the fly. 

Lastly, while we have been working with a single application we have been able to avoid the larger question about the wider application environment the SPA will sit on. We haven't spent much time laying out a vision for what this thing looks like when it has five or six applications on it: what is the pattern of interaction, what should the philosophy being 'an application' be. In the last couple of weeks I was on the team, we started talking about Application 2, and these chickens came home to roost right quick. More thoughtfulness and ground work on this would've helped.

There is some other ticky-tack stuff that is not so great.

* We lose discipline around tickets and Jira releases rather quickly if we let it slide at all. 
* We do sometimes end up with a lot of things queued up in the "ready to deploy" column, which is not very CICD of us.
* We tend to change direction frequently. Partly this is just being reactive to the emerging business needs, but I feel it should be slightly more stable than it is. More than once, we have laid down a road map of the next few sprints, an it doesn't even make it to the end of the first iteration before being completely invalidated.
* Our versioning situation is rather confusing, with the correspondence between what's in Jira and what you see in the application info screen being not totally 1:1.

== Lessons learned

=== New product vs. existing product

The team really picked up momentum after we delivered the first major release of the SPA product in June. Having worked on other products 'from scratch' I feel that the gulf in difficulty between working on a product that doesn't have an established in production base and adding features to one that is already deployed in production is vast and underappreciated. The former is orders of magnitude more difficult.

This shows up most in time estimations. The tendency to underestimate effort required when you are building something totally from scratch is much, much greater. The likelihood that while building a new product you will encounter unforeseen challenges is much, much higher than if you're adding functionality to something that already exists.

I had expected that it would be be harder, but I have a better sense now of just _how much_ harder. This leads to several conclusions:

* The detail of your upfront design for a new product should be much greater than for an existing one - for the latter you can get away with less, and leave it to the devs to figure out. For the former, for each part in your initial scope, keep designing until you have the light-bulb moment when you say 'oh, there's actually a lot more work here than I thought' - it's almost certainly there.
* Even then, on your baseline estimate, pad it out more 
* Give a significantly wider cone of uncertainty when you are time estimating a new product than adding to an existing one
* Get something, anything, in production as fast as possible, so you can move from the 'new product' stage to 'adding features' stage earlier

=== The team as an ecosystem vs a machine, the fragile team equilibrium

My background is in operations. When I ran operations team, it was a feasible and useful metaphor to think of the team like a production line of machines, where work comes in one end, and is passed from workstation to workstation until the product is finished. I found this was helpful in examining and improving processes.

The team which is developing software in the way I've described doesn't work like this at all. Using this metaphor to decide how to treat the team will lead to disaster.

The new metaphor I have which is better suited to this sort of team is an ecosystem of different organisms. An ecosystem can be described mathematically as a time series of population numbers of the different species, with differential equations describing the interactions of the species.

What you tend to find in ecosystems is that at first you have wild fluctuations in populations, and you have very chaotic graphs. But over time the ecosystem settles down, and lands in state of equilibrium.

This is similar to what I observed with the team dynamic. At first it was very chaotic. People hadn't had time to get used to each other, the patterns of behavior weren't established. People rubbed each other the wrong way.

Over time, however, as people settled and got used to each others patterns and style of communication, things settled down we arrived eventually at a sort of equilibrium, and that's when things started ticking.

I see a couple of consequences for this for how you manage the team.

* Don't change the team. Every time you introduce or remove an element from the team, it's like adding or removing a species to an ecosystem. You don't just nudge the equilibrium off a bit, you throw it into new spasms, and there will be big fluctuations until eventually a new equilibrium point is reached, and that can take a long time, during which you will be much less productive.
* Keep an eye out for disruptive elements and remove them. Some ecosystems don't converge even when the elements are stable. Given the right conditions, they can fluctuate forever. In the context of a team, that means you never find an equilibrium point and you'll never be properly productive.

=== Momentum

Momentum is king. When you are delivering frequently, bringing in new requirements and pushing out releases, the devs gets into a flow state and everything goes right. 
Do everything you can to maintain that momentum. 

* Don't take on a chunk of work that is too big to deliver quickly. If you can't break it down, don't do it, or better yet, try harder to break it down.
* When requirements and priorities are changing rapidly, don't let that chaos get in front of the devs. Make sure by the time the iteration starts, the next two weeks is clean and comprehensible
* Jump on blockers relentlessly. They'll grind the team to a halt and break the flow state
* Design more up front, before it gets in front of the developers. The piece of work they will be working on should be summarizable in a sentence, and explainable in detail in a few pages. The design should still definitely change as the devs work on it, but you'll have a much more tangible base to work from

=== Scrum and roles as scaffolding while you figure out the dynamics

We never really had formal roles on the team. I was sort of like a scrum master, but I definitely got involved at least a bit in the 'what' and the 'how' elements of the design, and especially the business analysis. The head of software engineering was sort of the product owner, but did other things too. In fact to some degree we did everything, and we all had a say to some degree in what we did, how we did it, and how the team managed its work.

This is not how the scrum guide says to do things. By the letter of the law, this would be a bad state of affairs. But like I said before, once we found that equilibrium, I felt we were a pretty good embodiment of the agile ideal of a small, self-organizing team. The roles would've only added rigidity that we didn't need. That's more in line with what I would expect the founders of the movement would say is 'the goal', rather than these rather rigid roles.

I've always had this thought that the Scrum stuff was really a sort of scaffolding. When you're building a house, you put the scaffolding up so you're able to build shell of the structure. But as soon as you're able to, you _take down the scaffolding_. A role-less team that has found it's equilibrium is, I think, the goal. But this is the first time I've seen actual first hand evidence of it, and the experience has strengthened my view.

That does leave the question: would it have been better - would we have got to this state quicker - if we had been more explicit about roles from the beginning? I don't know, but I'm skeptical. The scaffolding does support, but it also _constrains_. Maybe it would be better to let the fluctuations happen, rather than try to tame them too early. 

I don't feel that this is any more than a theory at this point. But it would have an interesting consequence if its true. If the Scrum structure is not the goal, but rather the natural equilibrium of the self-organizing team is, _and_ an initial scrum structure only delays the arrival of that equilibrium, what good is Scrum at all?  

=== Having one focus

This I think is an obvious one. I believe anyone who has thought about it seriously, or read anything about the horrors of multitasking and context switching would tell you that it's a bad idea. The more I see it, the worse I think are its consequences.

But still on this team I heard repeatedly that it wasn't a practical possibility because of the competing expectations put on us. Or people underestimated the consequences of splitting someones time between two or more things.

I disagree with that. I think it's much more within our control than we think, and we should be much more aggressive about allowing each person, and each team, to focus on one thing at a time. I think the customers would be more understanding of our thinking, properly explained, than we give them credit for.

=== No part timers

Related to the above point, having part timers - those who have one foot in the team, but often need to dash off and do something else, potentially blocking the others - is catastrophic.

We should try extremely hard to group items that require a particular resource together into a single time box and force them to commit to working on those items _and only those items_ until they are done. Then they are free to go on their merry way and do as much multitasking as they like.

=== Technical debt

It's always worse than you think it will be when you incur it. Be very hesitant to do it.

It's inevitable you _will_ have to incur some at some point, and in some situations it's the right decision. But give yourself a line in the sand, where you'll put your foot down and say "We can't cut this corner". The learning from this product is that CICD is that line in the sand, at least for a particular type of software - we should almost never agree to launch a piece of software and say the CICD will come later.

If you have TD, make sure you are paying some off in every iteration.

Nothing revolutionary here, but my time on this team certainly strengthened my resolve on this point.

=== The efficacy of planning

I don't think there was any point in my time on the team where what we thought we would be working on in a months time was what we actually ended up working on when that month rolled around. More proof, if it were needed, that plans are useless. But planning is maybe still essential. I need to think a lot more about this.

=== Upfront design

It's good! You shouldn't go into an iteration without at least a pretty decent design. Whenever we tried it, it never went well.

In the future, I'm planning to go into every iteration with some fairly detailed plan of what we'll be doing in it. Stories are not enough. A Sprint Goal is not enough. Between 2 and 4 pages of writing, diagrams and lo-fi mockups for a 2 week sprint feels about right.

=== Motivating developers

This is obviously going to be super generalized and simplistic.

Coming from a business world, it can be hard to get a dev engaged and excited in the same way you would someone on your business team. They have different motivations, likely a different personality, they get excited by different things - that's why they're a software engineer not something else.

Most software engineers I've worked with don't care about impressing the CEO. They don't care about getting a bigger budget. They want to work on things that are interesting to them. They want to build things. They want to see things they've coded running on peoples screens. They want to solve problems and puzzles. They often have a sense of the "right way" to build things, and if they see something that is not the "right way" they will be motivated to fix it.

If you walk up to a team of software engineers and say "We have a request from the CEO, this is a great opportunity to impress him, and if we do we'll get a bigger budget for next year", and are shocked when they don't seem to bubble with excitement, you need to rethink your approach.

Instead, sell them on the _product_. Talk about the impact it will have. Pick out some of the gnarlier problems we'll need to solve and throw them out there. Bring them some designs. Get them solving the puzzle in their heads. Bring them the problem that people are trying to solve and watch as they try to figure out the 'right way' to design the UX for that.

When you're working with a dev in an ongoing project (assuming you're not also a dev), figure out their nerd-buttons and press them. Then steer that to the product you're working on. Be careful to reign them in when they take it too far - and they will. I had a dev on this team propose in all seriousness that we re-write an entire application which we had just finished, with no additional functionality, in a totally different language which he had never written a line of code in, for some negligible difference in the floating point numbers implementation that would have zero practical impact.

=== Creating distance

This is a bit of a re-hash of what I said above: You should be aiming to have the team talk openly and non-judgementally to each other. But throwing this wide open can allow bad dynamics can develop. People can be critical or dismissive of the ideas of others. People can give small, subconscious signs of disrespect. Over time, if you let them, these can get more overt and the situation can spiral out of control until there's no possibility of constructive discussion.

Obviously it's best to catch these early and nix them.

What we did in this situation, like I said, was to put an intermediary between the parties. This was a short term fix, I don't know whether it could be resolved in the long run or if you should give up and remove one of them from the team.

=== Contracted resources don't mean you are hands off.

Having contracted devs forces you to decide to either dedicate an in-house resource to managing that contractor for the entire period of their engagement (and it's a _lot of work_), or to sacrifice any control you have over the design and quality of the code.

Even with our best contracted resource, we need to dedicate a significant portion of several of our peoples time to managing, directing and guiding the contractors. This is massively un-scalable.

In addition, contractors are by definition temporary. As we saw above, it took a long time for the team to gel, and get to a point of decent productivity. When you are there, changes are extremely disruptive. Having contractors roll on and off is extremely detrimental to this.

What we should be doing is setting up small teams that can work autonomously on applications without the need for constant oversight, and can work together for long enough to gel as a team _and then maintain that team for a long time afterwards_. That's not possible to do with contractors. So we should not use contractors for building new applications. 